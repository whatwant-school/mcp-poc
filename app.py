import os
import streamlit as st
from openai import OpenAI
import google.generativeai as genai

from dotenv import load_dotenv
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ê°€ì ¸ì™€ OpenAIì™€ Google Generative AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Streamlit ì•±ì˜ í˜ì´ì§€ ì„¤ì • êµ¬ì„±
st.set_page_config(page_title="Streamlit Chat", layout="centered")

# ì‚¬ìš©í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ì‚¬ì´ë“œë°”
model_name = st.sidebar.selectbox(
    "ğŸ§  ì‚¬ìš©í•  AI ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
    ["GPT-4", "GPT-3.5", "Gemini", "Custom-Bot"]
)

# í˜„ì¬ ì„ íƒëœ ëª¨ë¸ì„ ì‚¬ì´ë“œë°”ì— í‘œì‹œ
st.sidebar.markdown(f"í˜„ì¬ ì„ íƒëœ ëª¨ë¸: **{model_name}**")

# ì„¸ì…˜ ìƒíƒœì— ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ ì´ˆê¸°í™” (ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš°)
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë©”ì¸ ì œëª©
st.title("ğŸ’¬ ìŠ¤íŠ¸ë¦¼ë¦¿ ì±„íŒ… ë´‡")

# ì„¸ì…˜ ìƒíƒœì— ì €ì¥ëœ ëª¨ë“  ì´ì „ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# OpenAIì˜ GPT ëª¨ë¸ì—ì„œ ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_openai_response(prompt, model="gpt-3.5-turbo"):
    try:
        # OpenAI APIì— ì±„íŒ… ì™„ë£Œ ìš”ì²­ ë³´ë‚´ê¸°
        response = openai_client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        # ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µ ë°˜í™˜
        return response.choices[0].message.content.strip()
    except Exception as e:
        # API í˜¸ì¶œ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë°˜í™˜
        return f"âŒ OpenAI ì˜¤ë¥˜: {e}"

# Googleì˜ Gemini ëª¨ë¸ì—ì„œ ì‘ë‹µì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_gemini_response(prompt):
    try:
        # ìƒì„± ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì‘ë‹µ ìƒì„±
        model = genai.GenerativeModel("gemini-2.5-pro-exp-03-25")
        response = model.generate_content(prompt)
        # ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸ ë°˜í™˜
        return response.text
    except Exception as e:
        # API í˜¸ì¶œ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë°˜í™˜
        return f"âŒ Gemini ì˜¤ë¥˜: {e}"

# ì±„íŒ… ì…ë ¥ ìƒìì—ì„œ ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
    # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ ì‘ë‹µ ìƒì„±
    if model_name == "Gemini":
        response = get_gemini_response(prompt)
    elif model_name == "GPT-4":
        response = get_openai_response(prompt, model="gpt-4")
    else:
        response = get_openai_response(prompt, model="gpt-3.5-turbo")

    # ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.markdown(response)
